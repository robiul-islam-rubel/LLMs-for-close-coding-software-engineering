survey_response,llama_generated,human_codes
"I think that a push and shift toward increased documentation and the use of AIBOMs, or at least some of their components, will help report dependencies. This can be seen in the past 5 or so years, where there is a large increase of paper authors publishing code and including a list of hyperparameters, training recipes, software package versions, hardware used, and other details about experiments in papers. I think that as top venues start to require more of these documentations, there will be a shift to where publishing AIBOMs becomes the norm.",['Documentation'],"['IncreasedAdoption', 'Documentation', 'Requirements']"
I can only think of being able to reproduce the model to be completely sure of how it was created.,"['ReproduceModel', 'BadAnswer']",['ReproduceModel']
Should contain virtually everything mentioned in the last question. Even then reproducing results could be tricky.,['BadAnswer'],['BadAnswer']
"Having a sort of certification board or community, where open source AI software submit their projects for 3rd party assessment.",['Certification'],['Certification']
"Validation against third-party databases of possible dependencies, with the ability to add others not present in the registry only if users fill in structured data forms.","['Databases', 'AutomatedTool']",['Databases']
"Ensure that both c++ and python dependencies are exactly pinned to the version used for training, (enforcing that inference code needs to be available already when training) and provide a docker container that has the full environment provided already (so devs can debug possible differences to updated dependencies [e.g. using pytorch 2.0 instead of 1.12]).

Ensure that the model is created in a reproducible manner (e.g. trained on a fixed seed, while evaluating multiple training runs on different seeds)

Add easy support to domain drift detection, hence give the devs a possibility to quickly notice, if something went wrong in the data collection step and have the model exposed to data that is far away of the training data that is actually used.","['PinnedVersions', 'IncludedEnvironment', 'AutomatedTool']","['PinnedVersions', 'IncludedEnvironment', 'ReproduceModel']"
CICD action that runs some tests,"['BadAnswer', 'ContinuousIntegrationTests', 'AutomatedTool']",['ContinuousIntegrationTests']
"Establishing clear rules and best practices for the development and deployment of ML/DL systems, including a defined procedure for building and upgrading AIBOMs, is one way.",['EstablishBestPractices'],['EstablishBestPractices']
"Reproducable results from a trusted third party.
Validation with trusted datasets.
Certified vendor.",['Certification'],"['Certification', 'ReproduceModel']"
"Often it is as simple as providing the model code along with a requirements.txt file to specify dependencies.  Data must be provided in the same format as given to the model for training, and also in raw format where possible.  However, privacy concerns could trump reproducibility concerns at times.",['AutomatedTool'],"['ModelCode', 'ListLibraries', 'ReproduceModel']"
"Apart from software packages, dependencies of ML/DL systems include training/testing data which essentially defines the algorithm. I would emphasize on the reporting on the data dependencies. ",['EmphasisOnDataDependencies'],['EmphasisOnDataDependencies']
"Ensure model runs in a well-defined environment using tools such as Anaconda to automatically define exact package version dependencies.

Information about the training dataset should be kept track of somehow, though it's unclear to me how that could be done. Something similar to Huggingface Datasets that keeps track of the data itself as well as its provenance information (source, retrieval date, etc).","['PinnedVersions', 'AutomatedTool']",['EmphasisOnDataDependencies']
"1. Trusted 3rd party verification
2. Users/developers report false or incorrect dependencies",['Certification'],"['CommunityReporting', 'Certification']"
Provenance information,['ProvenanceInformation'],['ProvenanceInformation']
"Not sure if there is a way, at the moment. An automated tool for checking that what is reported in the AIBOM is correct and complete would be useful. I do not know to what extent that would be feasible, though.","['BadAnswer', 'AutomatedTool']",['AutomatedTool']
Universal API can provide this information. ,"['AutomatedTool', 'UniversalAPI']",['UniversalAPI']
"If the goal is full reproducibility, it is an open problem. If we settle for accountability and tracking for the purpose of security updates, I think it's do-able using existing enumerate the libraries-type approaches.",['AutomatedTool'],['ListLibraries']
"A module in the AIBOM should compare the ML model with the dependencies specified in the AIBOM.

OR

The ML model is trained via the AIBOM and therefore a mistake in the AIBOM would not allow the model to be used and vice-versa.
","['AutomatedTool', 'ModelTrainedViaAIBOM']","['ModelCode', 'ModelTrainedViaAIBOM', 'ListLibraries']"
Maybe a screening pipeline that validates and provide some certificates on the validity and requirements of AIBOMs ,"['AutomatedTool', 'ScreeningPipeline']",['ScreeningPipeline']
"'- Trust
- Clever application of zero-knowledge proofs
- Build data integrity with what Christian Catalini calls costless verification","['BadAnswer', 'ZeroKnowledgeProof', 'CostlessVerification']","['ZeroKnowledgeProof', 'CostlessVerification']"
