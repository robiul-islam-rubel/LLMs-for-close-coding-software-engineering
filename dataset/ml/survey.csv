survey_response,llama_generated,human_codes
"I think that a push and shift toward increased documentation and the use of AIBOMs, or at least some of their components, will help report dependencies. This can be seen in the past  or so years, where there is a large increase of paper authors publishing code and including a list of hyperparameters, training recipes, software package versions, hardware used, and other details about experiments in papers. I think that as top venues start to require more of these documentations, there will be a shift to where publishing AIBOMs becomes the norm.","['IncreasedAdoption', 'Documentation', 'Requirements', 'AutomatedTool']","['IncreasedAdoption', 'Documentation', 'Requirements']"
I can only think of being able to reproduce the model to be completely sure of how it was created.,"['ReproduceModel', 'ProvenanceInformation', 'AutomatedTool', 'ModelCode']",['ReproduceModel']
Should contain virtually everything mentioned in the last question. Even then reproducing results could be tricky.,"['Documentation', 'ReproduceModel', 'BadAnswer', 'PinnedVersions', 'EstablishBestPractices', 'ProvenanceInformation', 'AutomatedTool']",['BadAnswer']
"Having a sort of certification board or community, where open source AI software submit their projects for rd party assessment.","['Certification', 'EstablishBestPractices', 'ScreeningPipeline']",['Certification']
"Validation against thirdparty databases of possible dependencies, with the ability to add others not present in the registry only if users fill in structured data forms.","['Databases', 'EmphasisOnDataDependencies', 'AutomatedTool']",['Databases']
"Ensure that both c++ and python dependencies are exactly pinned to the version used for training, (enforcing that inference code needs to be available already when training) and provide a docker container that has the full environment provided already (so devs can debug possible differences to updated dependencies [e.g. using pytorch . instead of .]).Ensure that the model is created in a reproducible manner (e.g. trained on a fixed seed, while evaluating multiple training runs on different seeds)Add easy support to domain drift detection, hence give the devs a possibility to quickly notice, if something went wrong in the data collection step and have the model exposed to data that is far away of the training data that is actually used.","['PinnedVersions', 'IncludedEnvironment', 'EstablishBestPractices', 'AutomatedTool']","['PinnedVersions', 'IncludedEnvironment', 'ReproduceModel']"
CICD action that runs some tests,"['PinnedVersions', 'ContinuousIntegrationTests', 'AutomatedTool', 'ScreeningPipeline']",['ContinuousIntegrationTests']
"Establishing clear rules and best practices for the development and deployment of ML/DL systems, including a defined procedure for building and upgrading AIBOMs, is one way.","['Documentation', 'EstablishBestPractices']",['EstablishBestPractices']
Reproducable results from a trusted third party.Validation with trusted datasets.Certified vendor.,"['Certification', 'ProvenanceInformation']","['Certification', 'ReproduceModel']"
"Often it is as simple as providing the model code along with a requirements.txt file to specify dependencies.  Data must be provided in the same format as given to the model for training, and also in raw format where possible.  However, privacy concerns could trump reproducibility concerns at times.",['PinnedVersions'],"['ModelCode', 'ListLibraries', 'ReproduceModel']"
"Apart from software packages, dependencies of ML/DL systems include training/testing data which essentially defines the algorithm. I would emphasize on the reporting on the data dependencies. ",['EmphasisOnDataDependencies'],['EmphasisOnDataDependencies']
"Ensure model runs in a welldefined environment using tools such as Anaconda to automatically define exact package version dependencies.Information about the training dataset should be kept track of somehow, though its unclear to me how that could be done. Something similar to Huggingface Datasets that keeps track of the data itself as well as its provenance information (source, retrieval date, etc).","['PinnedVersions', 'EmphasisOnDataDependencies', 'ProvenanceInformation', 'AutomatedTool']",['EmphasisOnDataDependencies']
. Trusted rd party verification. Users/developers report false or incorrect dependencies,"['Certification', 'CommunityReporting', 'AutomatedTool']","['CommunityReporting', 'Certification']"
Provenance information,"['Documentation', 'Requirements', 'Certification', 'PinnedVersions', 'EstablishBestPractices', 'EmphasisOnDataDependencies', 'ProvenanceInformation', 'AutomatedTool', 'UniversalAPI', 'ScreeningPipeline', 'CostlessVerification']",['ProvenanceInformation']
"Not sure if there is a way, at the moment. An automated tool for checking that what is reported in the AIBOM is correct and complete would be useful. I do not know to what extent that would be feasible, though.","['AutomatedTool', 'ScreeningPipeline']",['AutomatedTool']
Universal API can provide this information.,"['Documentation', 'ProvenanceInformation', 'AutomatedTool', 'UniversalAPI']",['UniversalAPI']
"If the goal is full reproducibility, it is an open problem. If we settle for accountability and tracking for the purpose of security updates, I think its doable using existing enumerate the librariestype approaches.",[],['ListLibraries']
A module in the AIBOM should compare the ML model with the dependencies specified in the AIBOM.ORThe ML model is trained via the AIBOM and therefore a mistake in the AIBOM would not allow the model to be used and viceversa.,"['ProvenanceInformation', 'AutomatedTool', 'ModelTrainedViaAIBOM']","['ModelCode', 'ModelTrainedViaAIBOM', 'ListLibraries']"
Maybe a screening pipeline that validates and provide some certificates on the validity and requirements of AIBOMs,"['Certification', 'EstablishBestPractices', 'ProvenanceInformation', 'AutomatedTool', 'ScreeningPipeline']",['ScreeningPipeline']
Trust Clever application of zeroknowledge proofs Build data integrity with what Christian Catalini calls costless verification,"['BadAnswer', 'AutomatedTool', 'ZeroKnowledgeProof', 'CostlessVerification']","['ZeroKnowledgeProof', 'CostlessVerification']"
